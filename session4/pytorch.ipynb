{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  4.]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor from values\n",
    "a = torch.tensor([[1, 2, 4]], dtype=torch.float32)     # create tensor from values\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.3716929  0.10671763 0.67461187 0.69461477 0.50948256]\n",
      " [0.5664982  0.9036897  0.95842284 0.02542921 0.20762315]\n",
      " [0.2559032  0.82377505 0.99166316 0.55204743 0.4266114 ]\n",
      " [0.73056453 0.00121057 0.15028913 0.9993542  0.13688307]\n",
      " [0.22894198 0.43128413 0.1248633  0.17739904 0.76088506]]\n",
      "tensor([[ 0.3717,  0.1067,  0.6746,  0.6946,  0.5095],\n",
      "        [ 0.5665,  0.9037,  0.9584,  0.0254,  0.2076],\n",
      "        [ 0.2559,  0.8238,  0.9917,  0.5520,  0.4266],\n",
      "        [ 0.7306,  0.0012,  0.1503,  0.9994,  0.1369],\n",
      "        [ 0.2289,  0.4313,  0.1249,  0.1774,  0.7609]])\n"
     ]
    }
   ],
   "source": [
    "# tensor from numpy\n",
    "b_npy = np.random.random((5, 5)).astype(\"float32\")                     \n",
    "print(b_npy)\n",
    "b = torch.tensor(b_npy)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4761,  0.4938,  0.3167,  0.3423,  0.3792],\n",
       "        [ 0.1385,  0.3678,  0.1521,  0.0115,  0.3064],\n",
       "        [ 0.7492,  0.6215,  0.5232,  0.3266,  0.9820],\n",
       "        [ 0.6884,  0.2273,  0.7682,  0.7967,  0.2997],\n",
       "        [ 0.7273,  0.1180,  0.1513,  0.3336,  0.4257]])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# random tensors\n",
    "c = torch.rand(5, 5)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# zeros/ones tensors\n",
    "d = torch.zeros(3, 3)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.ones_like(d)\n",
    "e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some properties of tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whether it requires to compute gradient\n",
    "c.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# where the tensor is\n",
    "c.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what type the tensor has\n",
    "c.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# shape of the tensor\n",
    "c.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some operations on tensors\n",
    "see the docs: https://pytorch.org/docs/stable/tensors.html\n",
    "<br>\n",
    "also: https://pytorch.org/docs/stable/torch.html#math-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6487,  1.0225,  0.3342,  0.6757,  1.3462],\n",
       "        [ 1.4846,  1.1068,  1.1529,  0.4243,  1.1896],\n",
       "        [ 1.6959,  1.0366,  1.4236,  0.7783,  0.9389],\n",
       "        [ 1.6985,  0.9650,  0.4490,  0.6380,  0.9967],\n",
       "        [ 1.6503,  0.5376,  1.5316,  0.7680,  0.1247]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(5, 5, dtype=torch.float32)\n",
    "b = torch.rand_like(b, dtype=torch.float32, requires_grad=True)\n",
    "c = a + b\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.7181,  1.0454,  0.1117,  0.4566,  1.8122],\n",
       "        [ 2.2041,  1.2250,  1.3292,  0.1800,  1.4151],\n",
       "        [ 2.8759,  1.0745,  2.0267,  0.6058,  0.8815],\n",
       "        [ 2.8848,  0.9313,  0.2016,  0.4070,  0.9935],\n",
       "        [ 2.7233,  0.2890,  2.3459,  0.5899,  0.0156]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = c ** 2\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6.1441,  6.3534,  7.4645,  5.4183,  5.9636])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = d.sum(1)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Parameters are a special kind of tensors that will be used by optimizers in torch.optim\n",
    "\n",
    "Parameters have '.requires_grad' = True by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = torch.nn.Parameter(torch.randn(5, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(n.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modules are model building blocks in PyTorch (parameterized function).\n",
    "\n",
    "A module is created by subclassing torch.nn.Module.\n",
    "\n",
    "A module must register its parameters and submodules by attaching them directly onto self:\n",
    "(if you don't do this, parameters will not be found and thus will not be trained)\n",
    "\n",
    "** Rule #1 of Modules **: attach parameters and submodules onto self --> will get registered\n",
    "<br>\n",
    "** Rule #2 of Modules **: write a forward() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a custom module\n",
    "class TestModule(torch.nn.Module):\n",
    "    def __init__(self, indim, outdim, submodule=None):\n",
    "        super(TestModule, self).__init__()          # must call super()\n",
    "        self.linear = torch.nn.Linear(indim, outdim, bias=False)     # we attached a linear transformation layer to the module\n",
    "        self.W = torch.nn.Parameter(torch.randn(indim, outdim))      # we attached a parameter matrix to the module\n",
    "        self.submodule = submodule\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TestModule(5, 5)\n",
    "test2 = TestModule(5, 5, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TestModule(\n",
      "  (linear): Linear(in_features=5, out_features=5, bias=False)\n",
      ")\n",
      "TestModule(\n",
      "  (linear): Linear(in_features=5, out_features=5, bias=False)\n",
      "  (submodule): TestModule(\n",
      "    (linear): Linear(in_features=5, out_features=5, bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# printing the test shows the structure of the module\n",
    "print(test)   \n",
    "print(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[ 0.0874,  1.5442,  0.3271,  0.4075,  0.3752],\n",
       "         [-0.4018, -0.4171,  1.5578,  0.8214, -0.8434],\n",
       "         [-1.2709,  0.5951, -1.2376, -0.9346,  0.9792],\n",
       "         [ 0.8213, -0.7507, -1.2882,  0.9481,  0.1909],\n",
       "         [-1.1109, -0.5505, -0.1167,  0.2421, -0.4846]]), Parameter containing:\n",
       " tensor([[-0.2806, -0.2322,  0.3848,  0.0547,  0.1888],\n",
       "         [-0.3850, -0.3899, -0.3927, -0.0425, -0.2552],\n",
       "         [-0.3589,  0.2408, -0.4185, -0.0705,  0.2114],\n",
       "         [-0.2745, -0.1168,  0.1714,  0.0202, -0.0160],\n",
       "         [ 0.3096, -0.1182,  0.2436, -0.0324, -0.1903]])]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can also list the parameters\n",
    "list(test.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('W', Parameter containing:\n",
       "  tensor([[ 0.0874,  1.5442,  0.3271,  0.4075,  0.3752],\n",
       "          [-0.4018, -0.4171,  1.5578,  0.8214, -0.8434],\n",
       "          [-1.2709,  0.5951, -1.2376, -0.9346,  0.9792],\n",
       "          [ 0.8213, -0.7507, -1.2882,  0.9481,  0.1909],\n",
       "          [-1.1109, -0.5505, -0.1167,  0.2421, -0.4846]])),\n",
       " ('linear.weight', Parameter containing:\n",
       "  tensor([[-0.2806, -0.2322,  0.3848,  0.0547,  0.1888],\n",
       "          [-0.3850, -0.3899, -0.3927, -0.0425, -0.2552],\n",
       "          [-0.3589,  0.2408, -0.4185, -0.0705,  0.2114],\n",
       "          [-0.2745, -0.1168,  0.1714,  0.0202, -0.0160],\n",
       "          [ 0.3096, -0.1182,  0.2436, -0.0324, -0.1903]]))]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or parameters and their names/paths\n",
    "list(test.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('W', Parameter containing:\n",
       "  tensor([[-2.6009,  0.0731,  1.2403, -0.2455,  0.5811],\n",
       "          [ 0.5209, -0.0635, -0.5618,  0.2443,  1.4677],\n",
       "          [-0.3999,  1.3165,  0.1894,  0.9646,  0.0491],\n",
       "          [-0.5074,  1.8130,  0.2246, -0.8867, -0.7882],\n",
       "          [ 1.1245, -0.9600, -0.0222, -0.5023,  0.5953]])),\n",
       " ('linear.weight', Parameter containing:\n",
       "  tensor([[ 0.4421,  0.2815, -0.2408, -0.0181, -0.4305],\n",
       "          [ 0.4376,  0.0466, -0.0821, -0.1138, -0.3844],\n",
       "          [-0.1578, -0.4110,  0.3896,  0.0003, -0.2445],\n",
       "          [ 0.0517,  0.0818,  0.2739,  0.3353, -0.0179],\n",
       "          [-0.1306,  0.4023, -0.4120,  0.3429,  0.2351]])),\n",
       " ('submodule.W', Parameter containing:\n",
       "  tensor([[ 0.0874,  1.5442,  0.3271,  0.4075,  0.3752],\n",
       "          [-0.4018, -0.4171,  1.5578,  0.8214, -0.8434],\n",
       "          [-1.2709,  0.5951, -1.2376, -0.9346,  0.9792],\n",
       "          [ 0.8213, -0.7507, -1.2882,  0.9481,  0.1909],\n",
       "          [-1.1109, -0.5505, -0.1167,  0.2421, -0.4846]])),\n",
       " ('submodule.linear.weight', Parameter containing:\n",
       "  tensor([[-0.2806, -0.2322,  0.3848,  0.0547,  0.1888],\n",
       "          [-0.3850, -0.3899, -0.3927, -0.0425, -0.2552],\n",
       "          [-0.3589,  0.2408, -0.4185, -0.0705,  0.2114],\n",
       "          [-0.2745, -0.1168,  0.1714,  0.0202, -0.0160],\n",
       "          [ 0.3096, -0.1182,  0.2436, -0.0324, -0.1903]]))]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test2.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-- ** .forward() method of Modules **\n",
    "<br>\n",
    "This is called when you use the module.\n",
    "<br>\n",
    "Must implement logic here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Forward(torch.nn.Module):\n",
    "    ''' Single feedforward layer '''\n",
    "    def __init__(self, indim, outdim, bias=False, nonlinearity=torch.nn.functional.sigmoid):\n",
    "        super(Forward, self).__init__()\n",
    "        self.linear = torch.nn.Linear(indim, outdim, bias=False)\n",
    "        self.nonlinearity = nonlinearity\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ''' x must be of shape (batch_size, indim) '''\n",
    "        a = self.linear(x)\n",
    "        b = self.nonlinearity(a)\n",
    "        return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5415,  0.5421,  0.5795],\n",
       "        [ 0.6030,  0.5480,  0.6227]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 5)     # (two examples in a batch, 5 values in each example's vector)\n",
    "l = Forward(5, 3)        # (five input dimensions, three output dimensions)\n",
    "y = l(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# not that while we don't need to compute gradients of x, \n",
    "# we do need to compute gradients of y\n",
    "# in order to get gradients of the parameters in the submodules\n",
    "print(x.requires_grad)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "PyTorch does backpropagation for you.\n",
    "To compute gradients, call .backward() on some Tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([])\n",
      "tensor(3.4368)\n",
      "3.43684601784\n"
     ]
    }
   ],
   "source": [
    "# let's try a dummy loss (just a scalar based on our computations)\n",
    "dummy_loss = y.sum()\n",
    "print(dummy_loss.dtype)\n",
    "print(dummy_loss.size())    # --> it's a scalar !\n",
    "print(dummy_loss)\n",
    "print(dummy_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# let's check gradients of some parameters in our module\n",
    "print(l.linear.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3143,  0.3057,  0.4176,  0.2235,  0.1711],\n",
      "        [ 0.3217,  0.3110,  0.4243,  0.2277,  0.1762],\n",
      "        [ 0.3085,  0.3000,  0.4099,  0.2194,  0.1680]])\n"
     ]
    }
   ],
   "source": [
    "# let's check again\n",
    "print(l.linear.weight.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** now we have a gradient stored on the parameter Tensor !!! **\n",
    "\n",
    "PyTorch optimizers use .grad for updates on the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch NN\n",
    "PyTorch provides many different layers commonly used in neural networks.\n",
    "<br>\n",
    "We will only see a couple of them.\n",
    "<br>\n",
    "All such layers are in the torch.nn package: https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_words_in_voc = 1000\n",
    "embedding_dimension = 50\n",
    "embedder = torch.nn.Embedding(number_of_words_in_voc, \n",
    "                              embedding_dimension, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ids = torch.randint(0, number_of_words_in_voc, (3, 4), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 247,  137,  657,  784],\n",
      "        [ 616,  247,  439,   48],\n",
      "        [ 246,  662,   73,  158]])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "word_ids[1, 1] = word_ids[0, 0]\n",
    "print(word_ids)\n",
    "print(word_ids.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = embedder(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 50])\n"
     ]
    }
   ],
   "source": [
    "print(word_embeddings.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "print((word_embeddings[0, 0] - word_embeddings[1, 1]).norm())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_loss = word_embeddings.sum()\n",
    "dummy_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 784,  662,  657,  616,  439,  247,  246,  158,  137,   73,\n",
       "          48])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(embedder.weight.grad)[:, 0].unique()\n",
    "# gradient is non-zero only for the words we used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedfoward layer: see before"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = torch.nn.LSTM(50, 6, bidirectional=False, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3, 4, 50)       # three examples in a batch, 4 elements in sequence, each element is a five dim vector\n",
    "y = lstm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 6])\n",
      "torch.Size([3, 6])\n",
      "torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "print(y[0].size())\n",
    "y_T = y[1][0].squeeze(0)\n",
    "c_T = y[1][1].squeeze(0)\n",
    "print(c_T.size())\n",
    "print(y_T.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** combine **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, number_of_words_in_voc, (3, 4), dtype=torch.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.zero_grad()\n",
    "lstm.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 50])\n",
      "torch.Size([1, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings = embedder(x)\n",
    "print(embeddings.size())\n",
    "_, (final_state, _) = lstm(embeddings)\n",
    "print(final_state.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_loss = final_state.sum()\n",
    "dummy_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 905,  758,  703,  658,  580,  524,  427,  415,  335,  301,\n",
       "         209,   65])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(embedder.weight.grad)[:, 0].unique()\n",
    "# gradient is non-zero only for the words we used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 415,  209,  427,  301],\n",
      "        [ 335,  524,  703,  580],\n",
      "        [ 658,  905,   65,  758]])\n"
     ]
    }
   ],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm = torch.nn.LSTM(3, 4, bidirectional=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 8])\n",
      "torch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.random.manual_seed(1234)\n",
    "x = torch.randn(2, 6, 3, requires_grad=True)\n",
    "ys, (y_n, c_n) = bilstm(x)\n",
    "y_n = y_n.transpose(1, 0).contiguous().view(2, -1)\n",
    "print(ys.size())\n",
    "print(y_n.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       [[-0.0548, -0.1437, -0.0931, -0.3193,  0.0014, -8.1011],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 1. backprop on forward portion of y_n\n",
    "x.grad = None\n",
    "ys, (y_n, c_n) = bilstm(x)\n",
    "y_n = y_n.transpose(1, 0).contiguous().view(2, -1)\n",
    "l = y_n[0, :4].sum()\n",
    "l.backward()\n",
    "print(x.grad[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1401, -0.0894, -0.0242, -0.0064, -0.0081, -0.0049],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 2. backprop on backward portion of y_n\n",
    "x.grad = None\n",
    "ys, (y_n, c_n) = bilstm(x)\n",
    "y_n = y_n.transpose(1, 0).contiguous().view(2, -1)\n",
    "l = y_n[0, 4:].sum()\n",
    "l.backward()\n",
    "print(x.grad[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       [[-0.0548, -0.1437, -0.0931, -0.3193,  0.0014, -8.1011],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 3. backprop on forward portion of ys, t=N       (should be same as #1)\n",
    "x.grad = None\n",
    "ys, (y_n, c_n) = bilstm(x)\n",
    "y_n = y_n.transpose(1, 0).contiguous().view(2, -1)\n",
    "l = ys[0, -1, :4].sum()\n",
    "l.backward()\n",
    "print(x.grad[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1401, -0.0894, -0.0242, -0.0064, -0.0081, -0.0049],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 4. backprop on backward portion of ys, t=0      (should be same as #2)\n",
    "x.grad = None\n",
    "ys, (y_n, c_n) = bilstm(x)\n",
    "y_n = y_n.transpose(1, 0).contiguous().view(2, -1)\n",
    "l = ys[0, 0, 4:].sum()\n",
    "l.backward()\n",
    "print(x.grad[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -6.5817],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 5. backprop on backward portion of ys, t=N\n",
    "x.grad = None\n",
    "ys, (y_n, c_n) = bilstm(x)\n",
    "y_n = y_n.transpose(1, 0).contiguous().view(2, -1)\n",
    "l = ys[0, -1, 4:].sum()\n",
    "l.backward()\n",
    "print(x.grad[:, :, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-02 *\n",
      "       [[-6.8864,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])\n"
     ]
    }
   ],
   "source": [
    "# 6. backprop on forward portion of ys, t=0\n",
    "x.grad = None\n",
    "ys, (y_n, c_n) = bilstm(x)\n",
    "y_n = y_n.transpose(1, 0).contiguous().view(2, -1)\n",
    "l = ys[0, 0, :4].sum()\n",
    "l.backward()\n",
    "print(x.grad[:, :, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch RNNs and variable length sequences\n",
    "All torch.nn modules take batches of inputs. \n",
    "<br>\n",
    "We have variable length inputs so we pad the sequences (with zeros).\n",
    "<br>\n",
    "This is a problem because LSTM will also take into computation padded elements.\n",
    "<br>\n",
    "But there is a solution: PyTorch provides methods for packing and unpacking sequences.\n",
    "<br>\n",
    "We provide more convenient wrappers in lib.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import seq_pack, seq_unpack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  3.,  1.,  0.],\n",
       "        [ 4.,  4.,  0.,  0.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 3.,  0.,  0.,  0.],\n",
       "        [ 4.,  4.,  3.,  0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[2, 3, 1, 0],\n",
    "                  [4, 4, 0, 0],\n",
    "                  [1, 2, 3, 4],\n",
    "                  [3, 0, 0, 0],\n",
    "                  [4, 4, 3, 0]], dtype=torch.float32)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_x, unpack_order = seq_pack(x, x!= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1,  3,  0,  4,  2])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_unpacked, _= seq_unpack(packed_x, unpack_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.,  3.,  1.,  0.],\n",
       "        [ 4.,  4.,  0.,  0.],\n",
       "        [ 1.,  2.,  3.,  4.],\n",
       "        [ 3.,  0.,  0.,  0.],\n",
       "        [ 4.,  4.,  3.,  0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_unpacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usage with LSTMs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1,  1,  1,  0],\n",
       "        [ 1,  1,  0,  0],\n",
       "        [ 1,  1,  1,  1],\n",
       "        [ 1,  0,  0,  0],\n",
       "        [ 1,  1,  1,  0]], dtype=torch.uint8)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm = torch.nn.LSTM(4, 3, bidirectional=True, batch_first=True)\n",
    "embedder = torch.nn.Embedding(x.max().long().item()+1, 4)\n",
    "emb = embedder(x.long())\n",
    "emb = emb.detach()\n",
    "emb.requires_grad = True\n",
    "emb_mask = x != 0\n",
    "emb_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "packed_emb, unpack_order = seq_pack(emb, emb_mask)\n",
    "ys, (y_n, c_n) = bilstm(packed_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 3])\n",
      "tensor([ 1,  3,  0,  4,  2])\n"
     ]
    }
   ],
   "source": [
    "print(y_n.size())\n",
    "print(unpack_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 4, 6])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys, _ = seq_unpack(ys, unpack_order)\n",
    "ys.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 3])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n = y_n.index_select(1, unpack_order)\n",
    "y_n.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_n = y_n.transpose(1, 0).contiguous().view(5, -1)\n",
    "y_n.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_loss = y_n[1].sum()\n",
    "dummy_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0061,  0.3043,  0.2963,  0.1018],\n",
      "         [-0.0372,  0.2815,  0.1496,  0.0503],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(emb.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
