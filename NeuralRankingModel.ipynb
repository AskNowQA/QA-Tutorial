{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import tensor_utils as tu\n",
    "from utils import natural_language_utilities as nlutils\n",
    "\n",
    "#Torch related functionalities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = json.load(open('resources/dataset_with_paths.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convert URIs to their surface forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:38, 51.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Snippet for finding the surface forms of the hops in the dataset and adding it to the dataset.\n",
    "for index,d in tqdm(enumerate(dataset)):\n",
    "    hop1_sf = []\n",
    "    hop2_sf = []\n",
    "    \n",
    "    for hop1 in d['hop1']:\n",
    "        # ['+', 'http://dbpedia.org/ontology/office']\n",
    "        sf1 = nlutils.get_label_via_parsing(hop1[1],lower=True)\n",
    "        hop1_sf.append([hop1[0],sf1])\n",
    "    \n",
    "    for hop2 in d['hop2']:\n",
    "        # ['+', 'http://dbpedia.org/ontology/office', '-', 'http://dbpedia.org/property/isCitedBy']\n",
    "        sf1 = nlutils.get_label_via_parsing(hop2[1],lower=True)\n",
    "        sf2 = nlutils.get_label_via_parsing(hop2[3],lower=True)\n",
    "        hop2_sf.append([hop2[0],sf1,hop2[2],sf2])\n",
    "    \n",
    "    dataset[index]['hop1_sf'] = hop1_sf\n",
    "    dataset[index]['hop2_sf'] = hop2_sf\n",
    "    \n",
    "    # Convert Positive paths as well\n",
    "    dataset[index]['path_sf'] = [d['path'][0]]\n",
    "    dataset[index]['path_sf'].append(nlutils.get_label_via_parsing(d['path'][1],lower=True))\n",
    "    if len(d['path']) > 2:\n",
    "        dataset[index]['path_sf'].append(d['path'][2])\n",
    "        dataset[index]['path_sf'].append(nlutils.get_label_via_parsing(d['path'][3],lower=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'_pad_':0, '_unk_':1, '+':2, '-':3, '/':4, 'uri':5, 'x':6}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Base logic:\n",
    "vocab.setdefault('red', len(vocab))\n",
    "vocab.setdefault('blue', len(vocab))\n",
    "vocab.setdefault('red', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:03<00:00, 500.14it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(dataset):\n",
    "    \n",
    "    # Get words from the question\n",
    "    for token in d['node']['corrected_question'].split():\n",
    "        vocab.setdefault(token.lower(), len(vocab))\n",
    "        \n",
    "    # Get words from the positive path\n",
    "    for token in d['path_sf']:\n",
    "        for word in token.split(\" \"):\n",
    "            vocab.setdefault(word.lower(), len(vocab))\n",
    "        \n",
    "    # Get words from the negative paths\n",
    "    for path in d['hop1_sf'] + d['hop2_sf']:\n",
    "        for token in path:\n",
    "            for word in token.split(\" \"):\n",
    "                vocab.setdefault(word.lower(), len(vocab))\n",
    "\n",
    "itos = {value:key for key,value in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10279"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Trainable Data\n",
    "**Q, P, N** pairs. E.g.\n",
    "\n",
    "`Who is the president of USA`; `+ president`; `+ capital`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1 idfy everything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:04, 476.62it/s]\n"
     ]
    }
   ],
   "source": [
    "for index,d in tqdm(enumerate(dataset)):\n",
    "    \n",
    "    # Idfyng question\n",
    "    question_id = [vocab[word.lower()] for word in d['node']['corrected_question'].split()]\n",
    "    \n",
    "    # Idfying the positive path\n",
    "    positive_path_id = []\n",
    "    for token in d['path_sf']:\n",
    "        for word in token.split(\" \"):\n",
    "            positive_path_id.append(vocab[word.lower()])\n",
    "    \n",
    "    #idfying the generated negative path\n",
    "    hop1_id = []\n",
    "    \n",
    "    for path in d['hop1_sf']:\n",
    "        path_id = []\n",
    "        for tokens in path:\n",
    "            for word in tokens.split(\" \"):\n",
    "                path_id.append(vocab[word.lower()])\n",
    "        hop1_id.append(path_id)\n",
    "    \n",
    "    #idfying the generated negative path\n",
    "    hop2_id = []\n",
    "    \n",
    "    for path in d['hop2_sf']:\n",
    "        path_id = []\n",
    "        for tokens in path:\n",
    "            for word in tokens.split(\" \"):\n",
    "                path_id.append(vocab[word.lower()])\n",
    "        hop2_id.append(path_id)\n",
    "    \n",
    "    dataset[index]['question_id'] = question_id\n",
    "    dataset[index]['hop1_id'] = hop1_id\n",
    "    dataset[index]['hop2_id'] = hop2_id\n",
    "    dataset[index]['positive_path_id'] = positive_path_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2 split the data in train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = dataset[:int(len(dataset)*.80)],dataset[int(len(dataset)*.80):] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3 create triples Question,correct_path,incorrect_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of question, positive path and negative paths respecitvely are 1147053,1147053,1147053\n"
     ]
    }
   ],
   "source": [
    "train_question = []\n",
    "train_positive_path = []\n",
    "train_negative_path = []\n",
    "\n",
    "for d in train_data:\n",
    "    for path in d['hop1_id'] + d['hop2_id']:\n",
    "        train_question.append(d['question_id'])\n",
    "        train_positive_path.append(d['positive_path_id'])\n",
    "        train_negative_path.append(path)\n",
    "\n",
    "print(f\"length of question, positive path and negative paths respecitvely are \"\n",
    "      f\"{len(train_question)},{len(train_positive_path)},{len(train_negative_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    \"\"\"\n",
    "        Boilerplate class which helps others have some common functionality.\n",
    "        These are made with some debugging/loading and with corechains in mind\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def prepare_save(self):\n",
    "        pass\n",
    "\n",
    "    def load_from(self, location):\n",
    "        # Pull the data from disk\n",
    "        model_dump = torch.load(location)\n",
    "\n",
    "        # Load parameters\n",
    "        for key in self.prepare_save():\n",
    "            key[1].load_state_dict(model_dump[key[0]])\n",
    "\n",
    "    def get_parameter_sum(self):\n",
    "\n",
    "        sum = 0\n",
    "        for model in self.prepare_save():\n",
    "\n",
    "            model_sum = 0\n",
    "            for x in list(model[1].parameters()):\n",
    "\n",
    "                model_sum += np.sum(x.data.cpu().numpy().flatten())\n",
    "\n",
    "            sum += model_sum\n",
    "\n",
    "        return sum\n",
    "\n",
    "    def freeze_layer(self,layer):\n",
    "        for params in layer.parameters():\n",
    "            params.requires_grad = False\n",
    "\n",
    "    def unfreeze_layer(self,layer):\n",
    "        for params in layer.parameters():\n",
    "            params.requires_grad = True\n",
    "\n",
    "class BiLstmDot(Model):\n",
    "\n",
    "    def __init__(self, _parameter_dict, _word_to_id, _device, _pointwise=False, _debug=False):\n",
    "\n",
    "        self.debug = _debug\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "        self.pointwise = _pointwise\n",
    "        self.word_to_id = _word_to_id\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Init Models\")\n",
    "\n",
    "        self.encoder = NotSuchABetterEncoder(\n",
    "            number_of_layer=1,\n",
    "            bidirectional=self.parameter_dict['bidirectional'],\n",
    "            embedding_dim=self.parameter_dict['embedding_dim'],\n",
    "            max_length = self.parameter_dict['max_length'],\n",
    "            hidden_dim=self.parameter_dict['hidden_size'],\n",
    "            vocab_size=self.parameter_dict['vocab_size'],\n",
    "            dropout=self.parameter_dict['dropout'],\n",
    "            vectors=self.parameter_dict['vectors'],\n",
    "            enable_layer_norm=False,\n",
    "            mode = 'LSTM',\n",
    "            debug = self.debug).to(self.device)\n",
    "\n",
    "\n",
    "    def train(self, data, optimizer, loss_fn, device):\n",
    "        '''\n",
    "            Given data, passes it through model, inited in constructor, returns loss and updates the weight\n",
    "            :params data: {batch of question, pos paths, neg paths and dummy y labels}\n",
    "            :params optimizer: torch.optim object\n",
    "            :params loss fn: torch.nn loss object\n",
    "            :params device: torch.device object\n",
    "\n",
    "            returns loss\n",
    "        '''\n",
    "\n",
    "        # Unpacking the data and model from args\n",
    "        ques_batch, pos_batch, neg_batch, y_label = data['ques_batch'], data['pos_batch'], data['neg_batch'], data['y_label']\n",
    "        neg_batch = tu.no_one_left_behind(neg_batch)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encoding all the data\n",
    "        hidden = self.encoder.init_hidden(ques_batch.shape[0],self.device)\n",
    "        _, ques_batch_encoded, _, _ = self.encoder(tu.trim(ques_batch), hidden)\n",
    "        _, pos_batch_encoded, _, _ = self.encoder(tu.trim(pos_batch), hidden)\n",
    "        _, neg_batch_encoded, _, _  = self.encoder(tu.trim(neg_batch), hidden)\n",
    "\n",
    "        # Calculating dot score\n",
    "        pos_scores = torch.sum(ques_batch_encoded * pos_batch_encoded, -1)\n",
    "        neg_scores = torch.sum(ques_batch_encoded * neg_batch_encoded, -1)\n",
    "\n",
    "        '''\n",
    "            If `y == 1` then it assumed the first input should be ranked higher\n",
    "            (have a larger value) than the second input, and vice-versa for `y == -1`\n",
    "        '''\n",
    "        try:\n",
    "            loss = loss_fn(pos_scores, neg_scores, y_label)\n",
    "        except RuntimeError:\n",
    "            traceback.print_exc()\n",
    "            print(pos_scores.shape, neg_scores.shape, y_label.shape,  ques_batch.shape, pos_batch.shape, neg_batch.shape)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.encoder.parameters(), .5)\n",
    "        optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def predict(self, ques, paths, device):\n",
    "        \"\"\"\n",
    "            prediction function.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "\n",
    "            self.encoder.eval()\n",
    "            hidden = self.encoder.init_hidden(ques.shape[0], self.device)\n",
    "\n",
    "            _, question, _, _ = self.encoder(tu.trim(ques.long()), hidden)\n",
    "            _, paths, _, _ = self.encoder(tu.trim(paths.long()), hidden)\n",
    "\n",
    "            if self.pointwise:\n",
    "                score = torch.sum(question * paths, -1)\n",
    "            else:\n",
    "                score = torch.sum(question * paths, -1)\n",
    "\n",
    "            self.encoder.train()\n",
    "            return score\n",
    "\n",
    "    def prepare_save(self):\n",
    "        \"\"\"\n",
    "\n",
    "            This function is called when someone wants to save the underlying models.\n",
    "            Returns a tuple of key:model pairs which is to be interpreted within save model.\n",
    "\n",
    "        :return: [(key, model)]\n",
    "        \"\"\"\n",
    "        return [('encoder', self.encoder)]\n",
    "\n",
    "    def load_from(self, location):\n",
    "        # Pull the data from disk\n",
    "        if self.debug: print(\"loading Bilstmdot model from\", location)\n",
    "        self.encoder.load_state_dict(torch.load(location)['encoder'])\n",
    "        if self.debug: print(\"model loaded with weights ,\", self.get_parameter_sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotSuchABetterEncoder(nn.Module):\n",
    "    def __init__(self, max_length, hidden_dim, number_of_layer,\n",
    "                 embedding_dim, vocab_size, bidirectional,\n",
    "                 dropout=0.0, mode='LSTM', enable_layer_norm=False,\n",
    "                 vectors=None, debug=False, residual=False):\n",
    "        '''\n",
    "            :param max_length: Max length of the sequence.\n",
    "            :param hidden_dim: dimension of the output of the LSTM.\n",
    "            :param number_of_layer: Number of LSTM to be stacked.\n",
    "            :param embedding_dim: The output dimension of the embedding layer/ important only if vectors=none\n",
    "            :param vocab_size: Size of vocab / number of rows in embedding matrix\n",
    "            :param bidirectional: boolean - if true creates BIdir LStm\n",
    "            :param vectors: embedding matrix\n",
    "            :param debug: Bool/ prints shapes and some other meta data.\n",
    "            :param enable_layer_norm: Bool/ layer normalization.\n",
    "            :param mode: LSTM/GRU.\n",
    "            :param residual: Bool/ return embedded state of the input.\n",
    "\n",
    "        TODO: Implement multilayered shit someday.\n",
    "        '''\n",
    "        super(NotSuchABetterEncoder, self).__init__()\n",
    "\n",
    "        self.max_length, self.hidden_dim, self.embedding_dim, self.vocab_size = int(max_length), int(hidden_dim), int(embedding_dim), int(vocab_size)\n",
    "        self.enable_layer_norm = enable_layer_norm\n",
    "        self.number_of_layer = number_of_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        self.debug = debug\n",
    "        self.mode = mode\n",
    "        self.residual = residual\n",
    "\n",
    "\n",
    "        assert self.mode in ['LSTM', 'GRU']\n",
    "\n",
    "        if vectors is not None:\n",
    "            self.embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "            self.embedding_layer.weight.requires_grad = True\n",
    "        else:\n",
    "            # Embedding layer\n",
    "            self.embedding_layer = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # Mode\n",
    "        if self.mode == 'LSTM':\n",
    "            self.rnn = torch.nn.LSTM(input_size=self.embedding_dim,\n",
    "                                     hidden_size=self.hidden_dim,\n",
    "                                     num_layers=1,\n",
    "                                     bidirectional=self.bidirectional)\n",
    "        elif self.mode == 'GRU':\n",
    "            self.rnn = torch.nn.GRU(input_size=self.embedding_dim,\n",
    "                                    hidden_size=self.hidden_dim,\n",
    "                                    num_layers=1,\n",
    "                                    bidirectional=self.bidirectional)\n",
    "        self.dropout = torch.nn.Dropout(p=self.dropout)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "            Hidden states to be put in the model as needed.\n",
    "        :param batch_size: desired batchsize for the hidden\n",
    "        :param device: torch device\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.mode == 'LSTM':\n",
    "            return (torch.ones((1+self.bidirectional , batch_size, self.hidden_dim), device=device),\n",
    "                    torch.ones((1+self.bidirectional, batch_size, self.hidden_dim), device=device))\n",
    "        else:\n",
    "            return torch.ones((1+self.bidirectional, batch_size, self.hidden_dim), device=device)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Here we reproduce Keras default initialization weights to initialize Embeddings/LSTM weights\n",
    "        \"\"\"\n",
    "        ih = (param for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for t in ih:\n",
    "            torch.nn.init.xavier_uniform_(t)\n",
    "        for t in hh:\n",
    "            torch.nn.init.orthogonal_(t)\n",
    "        for t in b:\n",
    "            torch.nn.init.constant_(t, 0)\n",
    "\n",
    "    def forward(self, x, h):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: input (batch, seq)\n",
    "        :param h: hiddenstate (depends on mode. see init hidden)\n",
    "        :param device: torch device\n",
    "        :return: depends on booleans passed @ init.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.debug:\n",
    "            print (\"\\tx:\\t\", x.shape)\n",
    "            if self.mode is \"LSTM\":\n",
    "                print (\"\\th[0]:\\t\", h[0].shape)\n",
    "            else:\n",
    "                print (\"\\th:\\t\", h.shape)\n",
    "\n",
    "        mask = tu.compute_mask(x)\n",
    "\n",
    "        x = self.embedding_layer(x).transpose(0, 1)\n",
    "\n",
    "        if self.debug: print (\"x_emb:\\t\\t\", x.shape)\n",
    "\n",
    "        if self.enable_layer_norm:\n",
    "            seq_len, batch, input_size = x.shape\n",
    "            x = x.view(-1, input_size)\n",
    "            x = self.layer_norm(x)\n",
    "            x = x.view(seq_len, batch, input_size)\n",
    "\n",
    "        if self.debug: print(\"x_emb bn:\\t\", x.shape)\n",
    "\n",
    "        # get sorted v\n",
    "        lengths = mask.eq(1).long().sum(1)\n",
    "        lengths_sort, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "\n",
    "        x_sort = x.index_select(1, idx_sort)\n",
    "        h_sort = (h[0].index_select(1, idx_sort), h[1].index_select(1, idx_sort)) \\\n",
    "            if self.mode is \"LSTM\" else h.index_select(1, idx_sort)\n",
    "\n",
    "        x_pack = torch.nn.utils.rnn.pack_padded_sequence(x_sort, lengths_sort)\n",
    "        x_dropout = self.dropout.forward(x_pack.data)\n",
    "        x_pack_dropout = torch.nn.utils.rnn.PackedSequence(x_dropout, x_pack.batch_sizes)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"\\nidx_sort:\", idx_sort.shape)\n",
    "            print(\"idx_unsort:\", idx_unsort.shape)\n",
    "            print(\"x_sort:\", x_sort.shape)\n",
    "            if self.mode is \"LSTM\":\n",
    "                print (\"h_sort[0]:\\t\\t\", h_sort[0].shape)\n",
    "            else:\n",
    "                print (\"h_sort:\\t\\t\", h_sort.shape)\n",
    "\n",
    "\n",
    "        o_pack_dropout, h_sort = self.rnn.forward(x_pack_dropout, h_sort)\n",
    "        o, _ = torch.nn.utils.rnn.pad_packed_sequence(o_pack_dropout)\n",
    "\n",
    "        # Unsort o based ont the unsort index we made\n",
    "        o_unsort = o.index_select(1, idx_unsort)  # Note that here first dim is seq_len\n",
    "        h_unsort = (h_sort[0].index_select(1, idx_unsort), h_sort[1].index_select(1, idx_unsort)) \\\n",
    "            if self.mode is \"LSTM\" else h_sort.index_select(1, idx_unsort)\n",
    "\n",
    "\n",
    "        # @TODO: Do we also unsort h? Does h not change based on the sort?\n",
    "\n",
    "        if self.debug:\n",
    "            if self.mode is \"LSTM\":\n",
    "                print(\"h_sort\\t\\t\", h_sort[0].shape)\n",
    "            else:\n",
    "                print(\"h_sort\\t\\t\", h_sort.shape)\n",
    "            print(\"o_unsort\\t\\t\", o_unsort.shape)\n",
    "            if self.mode is \"LSTM\":\n",
    "                print(\"h_unsort\\t\\t\", h_unsort[0].shape)\n",
    "            else:\n",
    "                print(\"h_unsort\\t\\t\", h_unsort.shape)\n",
    "\n",
    "        len_idx = (lengths - 1).view(-1, 1).expand(-1, o_unsort.size(2)).unsqueeze(0)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"len_idx:\\t\", len_idx.shape)\n",
    "\n",
    "        # Need to also return the last embedded state. Wtf. How?\n",
    "\n",
    "        if self.residual:\n",
    "            len_idx = (lengths - 1).view(-1, 1).expand(-1, x.size(2)).unsqueeze(0)\n",
    "            x_last = x.gather(0, len_idx)\n",
    "            x_last = x_last.squeeze(0)\n",
    "            return o_unsort, h_unsort[0].transpose(1,0).contiguous().view(h_unsort[0].shape[1], -1) , h_unsort, mask, x, x_last\n",
    "        else:\n",
    "            return o_unsort, h_unsort[0].transpose(1,0).contiguous().view(h_unsort[0].shape[1], -1) , h_unsort, mask\n",
    "\n",
    "    @property\n",
    "    def layers(self):\n",
    "        return torch.nn.ModuleList([\n",
    "            torch.nn.ModuleList([self.embedding_layer, self.rnn, self.dropout]),\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/codes/conda/envs/qg/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(74.2771, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    parameter_dict = {}\n",
    "    parameter_dict['bidirectional'] = True\n",
    "    parameter_dict['embedding_dim'] = 300\n",
    "    parameter_dict['max_length'] = 25\n",
    "    parameter_dict['hidden_size'] = 256\n",
    "    parameter_dict['vocab_size'] = 1000\n",
    "    parameter_dict['dropout'] = 0.3\n",
    "    parameter_dict['vectors'] = None\n",
    "    model = BiLstmDot(parameter_dict, None, torch.device('cpu'), _pointwise=False, _debug=False)\n",
    "\n",
    "\n",
    "    # Data making \n",
    "    BS = 64 # Batch Size\n",
    "    SL = 25 # Maximum Sequence Length and also the sequence length of all tensor.\n",
    "    data = {}\n",
    "    data['ques_batch'] = torch.randint(0,999, (BS,SL), dtype=torch.long)\n",
    "    data['pos_batch'] = torch.randint(0,999, (BS,SL), dtype=torch.long)\n",
    "    data['neg_batch'] = torch.randint(0,999, (BS,SL), dtype=torch.long)\n",
    "    data['y_label'] = torch.ones(BS)\n",
    "\n",
    "    # Setting up optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(list(filter(lambda p: p.requires_grad, model.encoder.parameters())))\n",
    "    loss_fn = nn.MarginRankingLoss(margin=1,size_average=False)\n",
    "\n",
    "    # Passing it through models forward, train in this case.\n",
    "    output = model.train(data,optimizer,loss_fn,torch.device('cpu'))\n",
    "    \n",
    "    # Printing the output \n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestSampler:\n",
    "    \"\"\"\n",
    "        Given X and Y matrices (or lists of lists),\n",
    "            it returns a batch worth of stuff upon __next__\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, bs: int = 64):\n",
    "        '''\n",
    "            data has question, positive_path and negative paths as fields.\n",
    "        '''\n",
    "        self.question = data[\"question\"]\n",
    "        self.posp = data[\"positive_path\"]\n",
    "        self.negp = data[\"negative_path\"]\n",
    "        self.n = len(self.question)\n",
    "        self.bs = bs  # Batch Size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n // self.bs - (1 if self.n % self.bs else 0)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i, self.iter = 0, 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i + self.bs >= self.n:\n",
    "            raise StopIteration\n",
    "\n",
    "        _q, _p, _n = self.question[self.i:self.i + self.bs], \\\n",
    "        self.posp[self.i:self.i + self.bs], self.negp[self.i:self.i + self.bs]\n",
    "        self.i += self.bs\n",
    "        return _q, _p, _n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 4000 4000\n"
     ]
    }
   ],
   "source": [
    "# testing the sampler\n",
    "data = {\n",
    "    \"question\":train_question,\n",
    "    \"positive_path\": train_positive_path,\n",
    "    \"negative_path\": train_negative_path\n",
    "}\n",
    "ss = SimplestSampler(data,4000)\n",
    "\n",
    "for i,j,k in ss:\n",
    "    print(len(i),len(j),len(k))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(data_list, max_length = 25):\n",
    "    \n",
    "    padded_data = np.zeros((len(data_list),max_length))\n",
    "    \n",
    "    for ind,d in enumerate(data_list):\n",
    "        padded_data[ind][:min(len(padded_data[ind]),len(d))] = d[:min(len(d),len(padded_data[ind]))]\n",
    "\n",
    "    return padded_data\n",
    "\n",
    "def evaluation(data, modeler, max_length, device):\n",
    "    # We will follow the same procedure as that of training data preparation\n",
    "    \n",
    "    precision = []\n",
    "    \n",
    "    for d in data:\n",
    "        ques, posp, negp = d['question_id'], d['positive_path_id'], d['hop1_id']+d['hop2_id']\n",
    "        \n",
    "        # padding the data\n",
    "        negp_p = pad(negp,max_length)\n",
    "        posp_p = pad([posp],max_length)\n",
    "        ques_p = pad([ques],max_length)\n",
    "        \n",
    "        # stack negative path and positive path.\n",
    "        # The reason to stack them is that the model while predicting expects just a set of paths.\n",
    "        paths = np.vstack((negp_p,posp_p))\n",
    "        \n",
    "        # Repeating question \"path\" number of times.\n",
    "        ques_p = np.repeat(ques_p, len(paths), axis=0)\n",
    "        \n",
    "        # Converting them into pytorch tensor.\n",
    "        ques_p = torch.tensor(ques_p, dtype=torch.long, device=device)\n",
    "        paths = torch.tensor(paths, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Passing the data through predict function. \n",
    "        score = modeler.predict(ques_p, paths, device)\n",
    "        \n",
    "        # Find the index of highest scoring question-corechain (path) pair.\n",
    "        arg_max = torch.argmax(score)\n",
    "        \n",
    "        if arg_max.item() == len(paths)-1:\n",
    "            precision.append(1)\n",
    "        else:\n",
    "            precision.append(0)\n",
    "            \n",
    "    print(f\"the current precision of the system is, {np.average(precision)}\")\n",
    "        \n",
    "def training_loop(parameter_dict, train_loader, modeler, optimizer, loss_func, test_data, device):\n",
    "    '''\n",
    "        parameter_dict['epochs'] = 10\n",
    "    \n",
    "    '''\n",
    "    for epoch in range(parameter_dict['epochs']):\n",
    "        \n",
    "        print(\"Epoch: \", epoch, \"/\", parameter_dict['epochs'])\n",
    "        \n",
    "        epoch_loss = []\n",
    "        epoch_time = time.time()\n",
    "        \n",
    "        accuracy = evaluation(test_data, modeler, parameter_dict['max_length'], device)\n",
    "        print(accuracy)\n",
    "        \n",
    "        i_batch = 0\n",
    "        for q_b, pp_b, np_b in train_loader:\n",
    "            \n",
    "            \n",
    "            batch_time = time.time()\n",
    "            \n",
    "            # pad them\n",
    "            q_b, pp_b, np_b = pad(q_b,parameter_dict['max_length']),\\\n",
    "            pad(pp_b,parameter_dict['max_length']), pad(np_b,parameter_dict['max_length'])\n",
    "            \n",
    "            # convert them into torch tensor\n",
    "            \n",
    "            ques_batch = torch.tensor(np.reshape(q_b, (-1, parameter_dict['max_length'])),\n",
    "                                              dtype=torch.long, device=device)\n",
    "            pos_batch = torch.tensor(np.reshape(pp_b, (-1, parameter_dict['max_length'])),\n",
    "                                             dtype=torch.long, device=device)\n",
    "            neg_batch = torch.tensor(np.reshape(np_b, (-1, parameter_dict['max_length'])),\n",
    "                                             dtype=torch.long, device=device)\n",
    "            y = torch.ones(q_b.shape[0], device=device) #check if view(-1) is necessary\n",
    "            \n",
    "            # pass it through model\n",
    "            \n",
    "            data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'pos_batch': pos_batch,\n",
    "                            'neg_batch': neg_batch,\n",
    "                            'y_label': y\n",
    "            }\n",
    "            \n",
    "            loss = modeler.train(data=data_batch,\n",
    "                                  optimizer=optimizer,\n",
    "                                  loss_fn=loss_func,\n",
    "                                  device=device)\n",
    "            epoch_loss.append(loss.item())\n",
    "            \n",
    "            \n",
    "            print(\"Batch:\\t%d\" % i_batch, \"/%d\\t: \" % (parameter_dict['batch_size']),\n",
    "                      \"%s\" % (time.time() - batch_time),\n",
    "                      \"\\t%s\" % (time.time() - epoch_time),\n",
    "                      \"\\t%s\" % (str(loss.item())),\n",
    "                      end=None if i_batch + 1 == int(int(i_batch) / parameter_dict['batch_size']) else \"\\n\")\n",
    "            \n",
    "            i_batch = i_batch + 1\n",
    "        accuracy = evaluation(test_data, modeler, parameter_dict['max_length'], device)\n",
    "        print(accuracy)\n",
    "#         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurav/codes/conda/envs/qg/lib/python3.6/site-packages/torch/nn/functional.py:52: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 5\n",
      "the current precision of the system is, 0.075\n",
      "None\n",
      "Batch:\t0 /4000\t:  5.197627544403076 \t85.30159902572632 \t3830.44482421875\n",
      "Batch:\t1 /4000\t:  5.387554407119751 \t90.68939280509949 \t4347.18212890625\n",
      "Batch:\t2 /4000\t:  3.4218385219573975 \t94.11154127120972 \t823.1513061523438\n",
      "Batch:\t3 /4000\t:  4.2060205936431885 \t98.31787896156311 \t1646.2049560546875\n",
      "Batch:\t4 /4000\t:  4.0779993534088135 \t102.3963212966919 \t314.9248352050781\n",
      "Batch:\t5 /4000\t:  4.269907236099243 \t106.66671586036682 \t1758.103515625\n",
      "Batch:\t6 /4000\t:  4.062440872192383 \t110.72955870628357 \t420.0933532714844\n",
      "Batch:\t7 /4000\t:  4.854046583175659 \t115.58400654792786 \t3784.54931640625\n",
      "Batch:\t8 /4000\t:  3.9621925354003906 \t119.54663109779358 \t339.4937744140625\n",
      "Batch:\t9 /4000\t:  3.4537179470062256 \t123.00073504447937 \t353.0498962402344\n",
      "Batch:\t10 /4000\t:  5.024760484695435 \t128.0258984565735 \t1223.7586669921875\n",
      "Batch:\t11 /4000\t:  4.64821195602417 \t132.67453527450562 \t1450.707763671875\n",
      "Batch:\t12 /4000\t:  4.490438222885132 \t137.16538906097412 \t918.9917602539062\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-a482840d1d1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m training_loop(parameter_dict=parameter_dict, train_loader=ss, modeler=model, \n\u001b[0;32m---> 32\u001b[0;31m               optimizer=optimizer, loss_func=loss_fn, test_data=test_data,device = device)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-650b48d4096d>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(parameter_dict, train_loader, modeler, optimizer, loss_func, test_data, device)\u001b[0m\n\u001b[1;32m     92\u001b[0m                                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                                   \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                                   device=device)\n\u001b[0m\u001b[1;32m     95\u001b[0m             \u001b[0mepoch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-6a9530a5f9ac>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, data, optimizer, loss_fn, device)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_exc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mques_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/conda/envs/qg/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/codes/conda/envs/qg/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_dict = {}\n",
    "parameter_dict['bidirectional'] = True\n",
    "parameter_dict['embedding_dim'] = 300\n",
    "parameter_dict['max_length'] = 25\n",
    "parameter_dict['hidden_size'] = 256\n",
    "parameter_dict['vocab_size'] = len(vocab)\n",
    "parameter_dict['dropout'] = 0.3\n",
    "parameter_dict['vectors'] = None\n",
    "parameter_dict['epochs'] = 5\n",
    "parameter_dict['batch_size'] = 4000\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = BiLstmDot(parameter_dict, None, device, _pointwise=False, _debug=False)\n",
    "    \n",
    "# Setting up optimizer and loss function\n",
    "optimizer = torch.optim.Adam(list(filter(lambda p: p.requires_grad, model.encoder.parameters())))\n",
    "loss_fn = nn.MarginRankingLoss(margin=1,size_average=False)\n",
    "\n",
    "# Sampler\n",
    "data = {\n",
    "    \"question\":train_question,\n",
    "    \"positive_path\": train_positive_path,\n",
    "    \"negative_path\": train_negative_path\n",
    "}\n",
    "\n",
    "ss = SimplestSampler(data,parameter_dict['batch_size'])\n",
    "\n",
    "\n",
    "# Training\n",
    "training_loop(parameter_dict=parameter_dict, train_loader=ss, modeler=model, \n",
    "              optimizer=optimizer, loss_func=loss_fn, test_data=test_data,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = test_data[1]\n",
    "max_length = 25\n",
    "ques, posp, negp = d['question_id'], d['positive_path_id'], d['hop1_id']+d['hop2_id']\n",
    "\n",
    "# padding the data\n",
    "negp_p = pad(negp,max_length)\n",
    "posp_p = pad([posp],max_length)\n",
    "ques_p = pad([ques],max_length)\n",
    "\n",
    "# stack negative path and positive path.\n",
    "# The reason to stack them is that the model while predicting expects just a set of paths.\n",
    "paths = np.vstack((negp_p,posp_p))\n",
    "\n",
    "# Repeating question \"path\" number of times.\n",
    "ques_p = np.repeat(ques_p, len(paths), axis=0)\n",
    "\n",
    "# Converting them into pytorch tensor.\n",
    "ques_p = torch.tensor(ques_p, dtype=torch.long, device=device)\n",
    "paths = torch.tensor(paths, dtype=torch.long, device=device)\n",
    "\n",
    "# Passing the data through predict function. \n",
    "score = model.predict(ques_p, paths, device)\n",
    "\n",
    "# Find the index of highest scoring question-corechain (path) pair.\n",
    "arg_max = torch.argmax(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
