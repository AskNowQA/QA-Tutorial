{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will discuss BiLSTM based pair wise ranking approach.\n",
    "\n",
    "We divide this into following major phases \n",
    "\n",
    "* **Data pre-processing** where the objective is to create (question,correct path, incorrect path) tuples.\n",
    "    * Creating core chain to their surface forms equivalent.\n",
    "    * Converting all the paths and question to an intenger space.\n",
    "    * Creating a tuple of (question, correct path, incorrect path) --> all in integer space. \n",
    "* **Neural Model Definition** where we intent to create a model which can take question and core chains and return two encoded vectors.\n",
    "* **Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import traceback\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import tensor_utils as tu\n",
    "from utils import natural_language_utilities as nlutils\n",
    "\n",
    "#Torch related functionalities\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "dataset = json.load(open('resources/dataset_with_paths.json'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Convert URIs to their surface forms in the core chains\n",
    "\n",
    "['+', 'http://dbpedia.org/ontology/office', '-' , 'http://dbpedia.org/ontology/presidentOf'] -> **`+ office - president of`**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:37, 52.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Snippet for finding the surface forms of the hops in the dataset and adding it to the dataset.\n",
    "for index,d in tqdm(enumerate(dataset)):\n",
    "    hop1_sf = []\n",
    "    hop2_sf = []\n",
    "    \n",
    "    for hop1 in d['hop1']:\n",
    "        # ['+', 'http://dbpedia.org/ontology/office']\n",
    "        sf1 = nlutils.get_label_via_parsing(hop1[1],lower=True)\n",
    "        hop1_sf.append([hop1[0],sf1])\n",
    "    \n",
    "    for hop2 in d['hop2']:\n",
    "        # ['+', 'http://dbpedia.org/ontology/office', '-', 'http://dbpedia.org/property/isCitedBy']\n",
    "        sf1 = nlutils.get_label_via_parsing(hop2[1],lower=True)\n",
    "        sf2 = nlutils.get_label_via_parsing(hop2[3],lower=True)\n",
    "        hop2_sf.append([hop2[0],sf1,hop2[2],sf2])\n",
    "    \n",
    "    dataset[index]['hop1_sf'] = hop1_sf\n",
    "    dataset[index]['hop2_sf'] = hop2_sf\n",
    "    \n",
    "    # Convert Positive paths as well\n",
    "    dataset[index]['path_sf'] = [d['path'][0]]\n",
    "    dataset[index]['path_sf'].append(nlutils.get_label_via_parsing(d['path'][1],lower=True))\n",
    "    if len(d['path']) > 2:\n",
    "        dataset[index]['path_sf'].append(d['path'][2])\n",
    "        dataset[index]['path_sf'].append(nlutils.get_label_via_parsing(d['path'][3],lower=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create a vocabulary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {'_pad_':0, '_unk_':1, '+':2, '-':3, '/':4, 'uri':5, 'x':6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:03<00:00, 490.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for d in tqdm(dataset):\n",
    "    \n",
    "    # Get words from the question\n",
    "    for token in d['node']['corrected_question'].split():\n",
    "        vocab.setdefault(token.lower(), len(vocab))\n",
    "        \n",
    "    # Get words from the positive path\n",
    "    for token in d['path_sf']:\n",
    "        for word in token.split(\" \"):\n",
    "            vocab.setdefault(word.lower(), len(vocab))\n",
    "        \n",
    "    # Get words from the negative paths\n",
    "    for path in d['hop1_sf'] + d['hop2_sf']:\n",
    "        for token in path:\n",
    "            for word in token.split(\" \"):\n",
    "                vocab.setdefault(word.lower(), len(vocab))\n",
    "\n",
    "itos = {value:key for key,value in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10279"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(itos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Create Trainable Data\n",
    "**Q, P, N** pairs. e.g.\n",
    "\n",
    "`Who is the president of USA`; `+ president`; `+ capital\n",
    " Who is the president of USA`; `+ president`; `- birthplace`\n",
    " \n",
    " ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1 idfy everything. \n",
    "\n",
    "e.g. Who is the president of USA -> 13, 24, 39, 21, 77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2000it [00:04, 490.24it/s]\n"
     ]
    }
   ],
   "source": [
    "for index,d in tqdm(enumerate(dataset)):\n",
    "    \n",
    "    # Idfyng question\n",
    "    question_id = [vocab[word.lower()] for word in d['node']['corrected_question'].split()]\n",
    "    \n",
    "    # Idfying the positive path\n",
    "    positive_path_id = []\n",
    "    for token in d['path_sf']:\n",
    "        for word in token.split(\" \"):\n",
    "            positive_path_id.append(vocab[word.lower()])\n",
    "    \n",
    "    #idfying the generated negative path\n",
    "    negative_paths_id = []\n",
    "    \n",
    "    for path in d['hop1_sf'] + d['hop2_sf']:\n",
    "        path_id = []\n",
    "        for tokens in path:\n",
    "            for word in tokens.split(\" \"):\n",
    "                path_id.append(vocab[word.lower()])\n",
    "        negative_paths_id.append(path_id)\n",
    "    \n",
    "    dataset[index]['question_id'] = question_id\n",
    "    dataset[index]['negative_paths_id'] = negative_paths_id\n",
    "    dataset[index]['positive_path_id'] = positive_path_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2 split the data in train and test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = dataset[:int(len(dataset)*.80)],dataset[int(len(dataset)*.80):] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3 create triples --> Question,correct_path,incorrect_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of question, positive path and negative paths respecitvely are 1147053, 1147053, 1147053\n"
     ]
    }
   ],
   "source": [
    "train_question = []\n",
    "train_positive_path = []\n",
    "train_negative_path = []\n",
    "\n",
    "for d in train_data:\n",
    "    for path in d['negative_paths_id']:\n",
    "        train_question.append(d['question_id'])\n",
    "        train_positive_path.append(d['positive_path_id'])\n",
    "        train_negative_path.append(path)\n",
    "\n",
    "print(f\"length of question, positive path and negative paths respecitvely are \"\n",
    "      f\"{len(train_question)}, {len(train_positive_path)}, {len(train_negative_path)}\")\n",
    "\n",
    "# @TODO: Shuffle the dataset (train.)\n",
    "index = np.arange(0,len(train_question))\n",
    "np.random.shuffle(index)\n",
    "\n",
    "# randomize\n",
    "train_question = [train_question[i] for i in index]\n",
    "train_positive_path = [train_positive_path[i] for i in index]\n",
    "train_negative_path = [train_negative_path[i] for i in index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Networks\n",
    "\n",
    "Language (Sentence) is sequential.\n",
    "\n",
    "We need a network which can process the inputs one word at a time, where the processing of future words depends upon the words already seen.\n",
    "\n",
    "\n",
    "We use recurrent networks for this purposes. But this isn't the only form of NNs which can work.\n",
    "\n",
    "![recurrent nets](img/rnnss.png)\n",
    "\n",
    "We use $h_6$, or the final output of the RNN as a vector _summarizing_ the sequence, and its encoded representation $\\vec{q}$ or $\\vec{c}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we use the `NotSuchABetterEncoder` class as the RNN, as defined below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NotSuchABetterEncoder(nn.Module):\n",
    "    def __init__(self, max_length, hidden_dim, number_of_layer,\n",
    "                 embedding_dim, vocab_size, bidirectional, device, \n",
    "                 dropout=0.0, mode='LSTM', enable_layer_norm=False,\n",
    "                 vectors=None, debug=False, residual=False):\n",
    "        '''\n",
    "            :param max_length: Max length of the sequence.\n",
    "            :param hidden_dim: dimension of the output of the LSTM.\n",
    "            :param number_of_layer: Number of LSTM to be stacked.\n",
    "            :param embedding_dim: The output dimension of the embedding layer/ important only if vectors=none\n",
    "            :param vocab_size: Size of vocab / number of rows in embedding matrix\n",
    "            :param bidirectional: boolean - if true creates BIdir LStm\n",
    "            :param vectors: embedding matrix\n",
    "            :param debug: Bool/ prints shapes and some other meta data.\n",
    "            :param enable_layer_norm: Bool/ layer normalization.\n",
    "            :param mode: LSTM/GRU.\n",
    "            :param residual: Bool/ return embedded state of the input.\n",
    "        '''\n",
    "        super(NotSuchABetterEncoder, self).__init__()\n",
    "\n",
    "        self.max_length, self.hidden_dim, self.embedding_dim, self.vocab_size = int(max_length), int(hidden_dim), int(embedding_dim), int(vocab_size)\n",
    "        self.enable_layer_norm = enable_layer_norm\n",
    "        self.number_of_layer = number_of_layer\n",
    "        self.bidirectional = bidirectional\n",
    "        self.dropout = dropout\n",
    "        self.debug = debug\n",
    "        self.mode = mode\n",
    "        self.residual = residual\n",
    "\n",
    "\n",
    "        assert self.mode in ['LSTM', 'GRU']\n",
    "\n",
    "        if vectors is not None:\n",
    "            self.embedding_layer = nn.Embedding.from_pretrained(torch.FloatTensor(vectors))\n",
    "            self.embedding_layer.weight.requires_grad = True\n",
    "        else:\n",
    "            # Embedding layer\n",
    "            self.embedding_layer = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "\n",
    "        # Mode\n",
    "        if self.mode == 'LSTM':\n",
    "            self.rnn = torch.nn.LSTM(input_size=self.embedding_dim,\n",
    "                                     hidden_size=self.hidden_dim,\n",
    "                                     num_layers=1,\n",
    "                                     bidirectional=self.bidirectional)\n",
    "        elif self.mode == 'GRU':\n",
    "            self.rnn = torch.nn.GRU(input_size=self.embedding_dim,\n",
    "                                    hidden_size=self.hidden_dim,\n",
    "                                    num_layers=1,\n",
    "                                    bidirectional=self.bidirectional)\n",
    "        self.dropout = torch.nn.Dropout(p=self.dropout)\n",
    "        self.device = device\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def init_hidden(self, batch_size, device):\n",
    "        \"\"\"\n",
    "            Hidden states to be put in the model as needed.\n",
    "        :param batch_size: desired batchsize for the hidden\n",
    "        :param device: torch device\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.mode == 'LSTM':\n",
    "            return (torch.ones((1+self.bidirectional , batch_size, self.hidden_dim), device=device),\n",
    "                    torch.ones((1+self.bidirectional, batch_size, self.hidden_dim), device=device))\n",
    "        else:\n",
    "            return torch.ones((1+self.bidirectional, batch_size, self.hidden_dim), device=device)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"\n",
    "        Here we reproduce Keras default initialization weights to initialize Embeddings/LSTM weights\n",
    "        \"\"\"\n",
    "        ih = (param for name, param in self.named_parameters() if 'weight_ih' in name)\n",
    "        hh = (param for name, param in self.named_parameters() if 'weight_hh' in name)\n",
    "        b = (param for name, param in self.named_parameters() if 'bias' in name)\n",
    "        for t in ih:\n",
    "            torch.nn.init.xavier_uniform_(t)\n",
    "        for t in hh:\n",
    "            torch.nn.init.orthogonal_(t)\n",
    "        for t in b:\n",
    "            torch.nn.init.constant_(t, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "\n",
    "        :param x: input (batch, seq)\n",
    "        :param h: hiddenstate (depends on mode. see init hidden)\n",
    "        :param device: torch device\n",
    "        :return: depends on booleans passed @ init.\n",
    "        \"\"\"\n",
    "        \n",
    "        # trimming data to remove extra trailing zeros (id for padding)\n",
    "        x = tu.trim(x)\n",
    "        \n",
    "        # Initialize a new hidden state\n",
    "        h = self.init_hidden(x.shape[0], self.device)\n",
    "\n",
    "        if self.debug:\n",
    "            print (\"\\tx:\\t\", x.shape)\n",
    "            if self.mode is \"LSTM\":\n",
    "                print (\"\\th[0]:\\t\", h[0].shape)\n",
    "            else:\n",
    "                print (\"\\th:\\t\", h.shape)\n",
    "\n",
    "        mask = tu.compute_mask(x)\n",
    "\n",
    "        x = self.embedding_layer(x).transpose(0, 1)\n",
    "\n",
    "        if self.debug: print (\"x_emb:\\t\\t\", x.shape)\n",
    "\n",
    "        if self.enable_layer_norm:\n",
    "            seq_len, batch, input_size = x.shape\n",
    "            x = x.view(-1, input_size)\n",
    "            x = self.layer_norm(x)\n",
    "            x = x.view(seq_len, batch, input_size)\n",
    "\n",
    "        if self.debug: print(\"x_emb bn:\\t\", x.shape)\n",
    "\n",
    "        # get sorted v\n",
    "        lengths = mask.eq(1).long().sum(1)\n",
    "        lengths_sort, idx_sort = torch.sort(lengths, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "\n",
    "        x_sort = x.index_select(1, idx_sort)\n",
    "        h_sort = (h[0].index_select(1, idx_sort), h[1].index_select(1, idx_sort)) \\\n",
    "            if self.mode is \"LSTM\" else h.index_select(1, idx_sort)\n",
    "\n",
    "        x_pack = torch.nn.utils.rnn.pack_padded_sequence(x_sort, lengths_sort)\n",
    "        x_dropout = self.dropout.forward(x_pack.data)\n",
    "        x_pack_dropout = torch.nn.utils.rnn.PackedSequence(x_dropout, x_pack.batch_sizes)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"\\nidx_sort:\", idx_sort.shape)\n",
    "            print(\"idx_unsort:\", idx_unsort.shape)\n",
    "            print(\"x_sort:\", x_sort.shape)\n",
    "            if self.mode is \"LSTM\":\n",
    "                print (\"h_sort[0]:\\t\\t\", h_sort[0].shape)\n",
    "            else:\n",
    "                print (\"h_sort:\\t\\t\", h_sort.shape)\n",
    "\n",
    "\n",
    "        o_pack_dropout, h_sort = self.rnn.forward(x_pack_dropout, h_sort)\n",
    "        o, _ = torch.nn.utils.rnn.pad_packed_sequence(o_pack_dropout)\n",
    "\n",
    "        # Unsort o based ont the unsort index we made\n",
    "        o_unsort = o.index_select(1, idx_unsort)  # Note that here first dim is seq_len\n",
    "        h_unsort = (h_sort[0].index_select(1, idx_unsort), h_sort[1].index_select(1, idx_unsort)) \\\n",
    "            if self.mode is \"LSTM\" else h_sort.index_select(1, idx_unsort)\n",
    "\n",
    "\n",
    "        # @TODO: Do we also unsort h? Does h not change based on the sort?\n",
    "\n",
    "        if self.debug:\n",
    "            if self.mode is \"LSTM\":\n",
    "                print(\"h_sort\\t\\t\", h_sort[0].shape)\n",
    "            else:\n",
    "                print(\"h_sort\\t\\t\", h_sort.shape)\n",
    "            print(\"o_unsort\\t\\t\", o_unsort.shape)\n",
    "            if self.mode is \"LSTM\":\n",
    "                print(\"h_unsort\\t\\t\", h_unsort[0].shape)\n",
    "            else:\n",
    "                print(\"h_unsort\\t\\t\", h_unsort.shape)\n",
    "\n",
    "        len_idx = (lengths - 1).view(-1, 1).expand(-1, o_unsort.size(2)).unsqueeze(0)\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"len_idx:\\t\", len_idx.shape)\n",
    "\n",
    "        # Need to also return the last embedded state. Wtf. How?\n",
    "\n",
    "        if self.residual:\n",
    "            len_idx = (lengths - 1).view(-1, 1).expand(-1, x.size(2)).unsqueeze(0)\n",
    "            x_last = x.gather(0, len_idx)\n",
    "            x_last = x_last.squeeze(0)\n",
    "            return o_unsort, h_unsort[0].transpose(1,0).contiguous().view(h_unsort[0].shape[1], -1) , h_unsort, mask, x, x_last\n",
    "        else:\n",
    "            return o_unsort, h_unsort[0].transpose(1,0).contiguous().view(h_unsort[0].shape[1], -1) , h_unsort, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[434, 212, 503, 542],\n",
      "        [470, 599, 347, 545],\n",
      "        [569, 202,  43, 922],\n",
      "        [ 84,  14, 981, 656],\n",
      "        [ 95,  46, 356, 956],\n",
      "        [416, 102, 908, 402],\n",
      "        [692,  51, 307,   3],\n",
      "        [189, 693, 706, 204],\n",
      "        [419, 297, 709, 392],\n",
      "        [503, 514, 956, 570]])\n",
      "Here, tensor([434, 212, 503, 542]) is a question, and there are 10 different questions.\n"
     ]
    }
   ],
   "source": [
    "## Initialing the encoder.\n",
    "vocab_size = 1000\n",
    "device = torch.device('cpu')\n",
    "\n",
    "encoder = NotSuchABetterEncoder(\n",
    "            number_of_layer=1,\n",
    "            bidirectional=True,\n",
    "            embedding_dim=300,\n",
    "            max_length = 25,\n",
    "            hidden_dim=7,\n",
    "            vocab_size=vocab_size,\n",
    "            dropout=0.5,\n",
    "            vectors=None,\n",
    "            enable_layer_norm=False,\n",
    "            mode = 'LSTM',\n",
    "            device = device,\n",
    "            debug = False).to(device)\n",
    "\n",
    "\n",
    "## Fake data to pass through it.\n",
    "BS = 10 # batch size\n",
    "SL = 4 # maximum sequence length\n",
    "\n",
    "sequence_batch = torch.randint(0,vocab_size,(BS,SL), dtype=torch.long)\n",
    "\n",
    "print(sequence_batch)\n",
    "print(f\"Here, {sequence_batch[0]} is a question, and there are {BS} different questions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0331,  0.0467, -0.5595,  0.3182,  0.1419, -0.3353, -0.4046, -0.0354,\n",
      "         -0.0706, -0.0597, -0.0012, -0.2459, -0.0696, -0.1162],\n",
      "        [-0.3064, -0.2185,  0.0032,  0.2066, -0.3276,  0.1606, -0.2801, -0.3562,\n",
      "         -0.0260,  0.0333, -0.0269,  0.1715,  0.3174,  0.5970],\n",
      "        [-0.2329,  0.4847, -0.1218, -0.5392, -0.0248,  0.0465, -0.1798,  0.1852,\n",
      "          0.6057,  0.1037, -0.0011,  0.0399,  0.2826, -0.1158],\n",
      "        [-0.0422,  0.1353, -0.4303,  0.1670,  0.1181, -0.0620,  0.0157, -0.0447,\n",
      "          0.0924,  0.3610,  0.1419, -0.1265, -0.0204, -0.0568],\n",
      "        [-0.0652,  0.0067,  0.2815,  0.0066, -0.0535, -0.1491,  0.1591,  0.2933,\n",
      "          0.0441, -0.5116,  0.6158, -0.0267,  0.0209,  0.6315],\n",
      "        [-0.1371, -0.1831, -0.2718, -0.0685,  0.2872, -0.2326,  0.1842,  0.7259,\n",
      "         -0.0537, -0.2072, -0.5387, -0.0334, -0.4122,  0.1925],\n",
      "        [ 0.3835, -0.3390,  0.3613,  0.0638, -0.3298, -0.0330, -0.3760, -0.0386,\n",
      "         -0.0575,  0.3747,  0.0124, -0.4666, -0.1919, -0.0738],\n",
      "        [ 0.0173,  0.0476,  0.1318, -0.8306,  0.1168,  0.1636,  0.3735, -0.1223,\n",
      "         -0.3478, -0.2030, -0.0323, -0.0249,  0.0490, -0.0838],\n",
      "        [-0.0568, -0.1028, -0.0440,  0.3028, -0.0897, -0.2079,  0.2082, -0.6255,\n",
      "         -0.3529, -0.2929,  0.0611,  0.0632,  0.0824,  0.4598],\n",
      "        [-0.3668, -0.1661,  0.0707, -0.1078, -0.0301, -0.3762,  0.0058,  0.1467,\n",
      "          0.1715, -0.1598,  0.1320,  0.0296,  0.1501, -0.0325]],\n",
      "       grad_fn=<ViewBackward>) torch.Size([10, 14])\n"
     ]
    }
   ],
   "source": [
    "# Passing the data to the model\n",
    "_, sequence_batch_encoded, _, _ = encoder(sequence_batch)\n",
    "print(sequence_batch_encoded, sequence_batch_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranking Architecture using RNNs\n",
    "Here the RNN acts as the Encoder\n",
    "\n",
    "![ranking architecture](img/archi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ranker():\n",
    "\n",
    "    def __init__(self, _parameter_dict, _device, _debug=False):\n",
    "\n",
    "        self.debug = _debug\n",
    "        self.parameter_dict = _parameter_dict\n",
    "        self.device = _device\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"Init Encoder\")\n",
    "\n",
    "        self.encoder = NotSuchABetterEncoder(\n",
    "            number_of_layer=1,\n",
    "            bidirectional=self.parameter_dict['bidirectional'],\n",
    "            embedding_dim=self.parameter_dict['embedding_dim'],\n",
    "            max_length = self.parameter_dict['max_length'],\n",
    "            hidden_dim=self.parameter_dict['hidden_size'],\n",
    "            vocab_size=self.parameter_dict['vocab_size'],\n",
    "            dropout=self.parameter_dict['dropout'],\n",
    "            vectors=self.parameter_dict['vectors'],\n",
    "            enable_layer_norm=False,\n",
    "            mode = 'LSTM',\n",
    "            device = _device,\n",
    "            debug = self.debug).to(self.device)\n",
    "        \n",
    "\n",
    "    def train(self, data):\n",
    "        '''\n",
    "            Given data, passes it through model, inited in constructor, returns loss and updates the weight\n",
    "            :params data: {batch of question, pos paths, neg paths and dummy y labels}\n",
    "            :params optimizer: torch.optim object\n",
    "            :params loss fn: torch.nn loss object\n",
    "            :params device: torch.device object\n",
    "\n",
    "            returns loss\n",
    "        '''\n",
    "\n",
    "        # Unpacking the data and model from args\n",
    "        ques_batch, pos_batch, neg_batch = data['ques_batch'], data['pos_batch'], data['neg_batch']\n",
    "        \n",
    "        # TODO: CAN WE DO WITHOUT THIS?\n",
    "        neg_batch = tu.no_one_left_behind(neg_batch)\n",
    "\n",
    "        \"\"\"\n",
    "            Encoding all the data\n",
    "        \"\"\"    \n",
    "            \n",
    "        # Encoding question,positive_path and negative path\n",
    "        _, ques_batch_encoded, _, _ = self.encoder(ques_batch)\n",
    "        _, pos_batch_encoded, _, _ = self.encoder(pos_batch)\n",
    "        _, neg_batch_encoded, _, _  = self.encoder(neg_batch)\n",
    "\n",
    "        # Calculating dot product\n",
    "        pos_scores = torch.sum(ques_batch_encoded * pos_batch_encoded, -1)\n",
    "        neg_scores = torch.sum(ques_batch_encoded * neg_batch_encoded, -1)\n",
    "\n",
    "        return pos_scores, neg_scores\n",
    "\n",
    "    def predict(self, ques, paths):\n",
    "        \"\"\"\n",
    "            prediction function.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Set mode to evaluate\n",
    "            self.encoder.eval()\n",
    "\n",
    "            _, question, _, _ = self.encoder(ques.long())\n",
    "            _, paths, _, _ = self.encoder(paths.long())\n",
    "\n",
    "            score = torch.sum(question * paths, -1)\n",
    "\n",
    "            # Set mode back to train\n",
    "            self.encoder.train()\n",
    "            \n",
    "            return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(85.7175, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    parameter_dict = {}\n",
    "    parameter_dict['bidirectional'] = True\n",
    "    parameter_dict['embedding_dim'] = 300\n",
    "    parameter_dict['max_length'] = 25\n",
    "    parameter_dict['hidden_size'] = 256\n",
    "    parameter_dict['vocab_size'] = 1000\n",
    "    parameter_dict['dropout'] = 0.3\n",
    "    parameter_dict['vectors'] = None\n",
    "    model = Ranker(parameter_dict, torch.device('cpu'), _debug=False)\n",
    "\n",
    "\n",
    "    # Data making \n",
    "    BS = 64 # Batch Size\n",
    "    SL = 25 # Maximum Sequence Length and also the sequence length of all tensor.\n",
    "    data = {}\n",
    "    data['ques_batch'] = torch.randint(0,999, (BS,SL), dtype=torch.long)\n",
    "    data['pos_batch'] = torch.randint(0,999, (BS,SL), dtype=torch.long)\n",
    "    data['neg_batch'] = torch.randint(0,999, (BS,SL), dtype=torch.long)\n",
    "\n",
    "    # Setting up optimizer and loss function\n",
    "    optimizer = torch.optim.Adam(list(filter(lambda p: p.requires_grad, model.encoder.parameters())))\n",
    "    loss_fn = nn.MarginRankingLoss(margin=1,size_average=False)\n",
    "    \n",
    "    # Cleanse the gradients (if any) from the model parameters\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Passing it through models forward, train in this case.\n",
    "    pos_scores, neg_scores = model.train(data)\n",
    "    \n",
    "    \"\"\"\n",
    "        **Explanation of y labels**\n",
    "        If `y == 1` then it assumed the first input should be ranked higher \n",
    "            (have a larger value) than the second input, \n",
    "            and vice-versa for `y == -1`. \n",
    "            \n",
    "        We want the pos scores to be higher than neg scores, and thus our Y label should always be 1.\n",
    "    \"\"\"\n",
    "    y_label = torch.ones(BS)\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = loss_fn(pos_scores, neg_scores, y_label)\n",
    "    \n",
    "    # Compute gradients to update parameters\n",
    "    loss.backward()\n",
    "    \n",
    "    # Use the gradients to update model parameters\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Printing the output \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sampler\n",
    "\n",
    "Create a mechanism to iterate over data, one batch at at time\n",
    "\n",
    "\n",
    "![sampler](https://media.giphy.com/media/H42kukGc8I1IlrC1fg/giphy.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimplestSampler:\n",
    "    \"\"\"\n",
    "        Given X and Y matrices (or lists of lists),\n",
    "            it returns a batch worth of stuff upon __next__\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data, bs: int = 64):\n",
    "        '''\n",
    "            Data has question, positive_path and negative paths as fields.\n",
    "        '''\n",
    "        self.question = data[\"question\"]\n",
    "        self.posp = data[\"positive_path\"]\n",
    "        self.negp = data[\"negative_path\"]\n",
    "        self.n = len(self.question)\n",
    "        self.bs = bs  # Batch Size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n // self.bs - (1 if self.n % self.bs else 0)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i, self.iter = 0, 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.i + self.bs >= self.n:\n",
    "            raise StopIteration\n",
    "\n",
    "        _q, _p, _n = self.question[self.i:self.i + self.bs], \\\n",
    "        self.posp[self.i:self.i + self.bs], self.negp[self.i:self.i + self.bs]\n",
    "        self.i += self.bs\n",
    "        return _q, _p, _n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000 4000 4000\n"
     ]
    }
   ],
   "source": [
    "# testing the sampler\n",
    "data = {\n",
    "    \"question\":train_question,\n",
    "    \"positive_path\": train_positive_path,\n",
    "    \"negative_path\": train_negative_path\n",
    "}\n",
    "ss = SimplestSampler(data,4000)\n",
    "\n",
    "for i,j,k in ss:\n",
    "    print(len(i),len(j),len(k))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Padding\n",
    "\n",
    "Questions, and core chains are not of the same length.\n",
    "E.g.\n",
    "\n",
    "```\n",
    "Q. Who is the chancellor of Germany?          (6 words)\n",
    "Q. Who is the mother     of Jon      Snow?    (7 words)\n",
    "```\n",
    "\n",
    "So when the RNN processes (and it does so one batch at a time), it'll stop at the shortest length question. I.e. \"snow\" will be ignored in our example.\n",
    "\n",
    "We thus pad the questions to be of the same length, in every batch.\n",
    "\n",
    "![unpadded batch](img/unpadded.png)\n",
    "\n",
    "turns to \n",
    "\n",
    "![padded batch](img/padded.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(data_list, max_length = 25):\n",
    "    \n",
    "    padded_data = np.zeros((len(data_list),max_length))\n",
    "    \n",
    "    for ind,d in enumerate(data_list):\n",
    "        padded_data[ind][:min(len(padded_data[ind]),len(d))] = d[:min(len(d),len(padded_data[ind]))]\n",
    "\n",
    "    return np.reshape(padded_data, (-1,max_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "![summing it all together](img/summing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(parameter_dict, data_sampler, ranker, optimizer, loss_func, test_data, device):\n",
    "    '''\n",
    "        parameter_dict['epochs'] = 10\n",
    "        # @TODO: Do we need a function or could it be simplified.\n",
    "    \n",
    "    '''\n",
    "    for epoch in range(parameter_dict['epochs']):\n",
    "        \n",
    "        print(\"Epoch: \", epoch, \"/\", parameter_dict['epochs'])\n",
    "        \n",
    "        epoch_loss = []\n",
    "        epoch_time = time.time()\n",
    "        i_batch = 0\n",
    "        \n",
    "        for q_b, pp_b, np_b in data_sampler:\n",
    "            \n",
    "            batch_time = time.time()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Step 1- Input data preparation.\n",
    "            \n",
    "            # Step 1a- Pad Data.\n",
    "            q_b = pad(q_b,parameter_dict['max_length'])\n",
    "            pp_b = pad(pp_b,parameter_dict['max_length'])\n",
    "            np_b = pad(np_b,parameter_dict['max_length'])\n",
    "            \n",
    "            # Step 1b- Convert data to Torch Tensors.\n",
    "            ques_batch = torch.tensor(q_b, dtype=torch.long, device=device)\n",
    "            pos_batch = torch.tensor(pp_b, dtype=torch.long, device=device)\n",
    "            neg_batch = torch.tensor(np_b, dtype=torch.long, device=device)\n",
    "            \n",
    "            # Step 2- Input data label.\n",
    "            y_label = torch.ones(q_b.shape[0], device=device)\n",
    "            \n",
    "            # Step 3- Model (already defined.)\n",
    "            \n",
    "            # Step 4- Model output (pass data through model.)\n",
    "            data_batch = {\n",
    "                            'ques_batch': ques_batch,\n",
    "                            'pos_batch': pos_batch,\n",
    "                            'neg_batch': neg_batch,\n",
    "            }\n",
    "            \n",
    "            pos_scores, neg_scores = ranker.train(data=data_batch)\n",
    "            \n",
    "            # Step 5- Compute Loss.\n",
    "            loss = loss_fn(pos_scores, neg_scores, y_label)\n",
    "\n",
    "            \n",
    "            # Step 6- Computing gradient.\n",
    "            loss.backward()\n",
    "\n",
    "            # Step 7- Parameter Update.\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss.append(loss.item())\n",
    "            \n",
    "            print(\"Batch:\\t%d\" % i_batch, \"/%d\\t: \" % (parameter_dict['batch_size']),\n",
    "                      \"%s\" % (time.time() - batch_time),\n",
    "                      \"\\t%s\" % (time.time() - epoch_time),\n",
    "                      \"\\t%s\" % (str(loss.item())),\n",
    "                      end=None if i_batch + 1 == int(int(i_batch) / parameter_dict['batch_size']) else \"\\n\")\n",
    "            \n",
    "            i_batch = i_batch + 1\n",
    "        \n",
    "        accuracy = evaluation(test_data, ranker, parameter_dict['max_length'], device)\n",
    "        \n",
    "        # TODO better printing here.\n",
    "        print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(data, modeler, max_length, device):\n",
    "    # We will follow the same procedure as that of training data preparation\n",
    "    \n",
    "    precision = []\n",
    "    \n",
    "    for d in data:\n",
    "        ques, posp, negp = d['question_id'], d['positive_path_id'], d['negative_paths_id']\n",
    "        \n",
    "        # padding the data\n",
    "        negp_p = pad(negp,max_length)\n",
    "        posp_p = pad([posp],max_length)\n",
    "        ques_p = pad([ques],max_length)\n",
    "        \n",
    "        # stack negative path and positive path.\n",
    "        # The reason to stack them is that the model while predicting expects just a set of paths.\n",
    "        paths = np.vstack((negp_p,posp_p))\n",
    "        \n",
    "        # Repeating question \"path\" number of times.\n",
    "        ques_p = np.repeat(ques_p, len(paths), axis=0)\n",
    "        \n",
    "        # Converting them into pytorch tensor.\n",
    "        ques_p = torch.tensor(ques_p, dtype=torch.long, device=device)\n",
    "        paths = torch.tensor(paths, dtype=torch.long, device=device)\n",
    "        \n",
    "        # Passing the data through predict function. \n",
    "        score = modeler.predict(ques_p, paths)\n",
    "        \n",
    "        # Find the index of highest scoring question-corechain (path) pair.\n",
    "        arg_max = torch.argmax(score)\n",
    "        \n",
    "        if arg_max.item() == len(paths)-1:\n",
    "            precision.append(1)\n",
    "        else:\n",
    "            precision.append(0)\n",
    "            \n",
    "    print(f\"the current precision of the system is, {np.average(precision)}\")\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 / 5\n",
      "Batch:\t0 /4000\t:  5.25373911857605 \t5.253835439682007 \t3384.860107421875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameter_dict = {}\n",
    "parameter_dict['bidirectional'] = True\n",
    "parameter_dict['embedding_dim'] = 300\n",
    "parameter_dict['max_length'] = 25\n",
    "parameter_dict['hidden_size'] = 256\n",
    "parameter_dict['vocab_size'] = len(vocab)\n",
    "parameter_dict['dropout'] = 0.3\n",
    "parameter_dict['vectors'] = None\n",
    "parameter_dict['epochs'] = 5\n",
    "parameter_dict['batch_size'] = 4000\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = Ranker(parameter_dict, device, _debug=False)\n",
    "    \n",
    "# Setting up optimizer and loss function\n",
    "optimizer = torch.optim.Adam(list(filter(lambda p: p.requires_grad, model.encoder.parameters())))\n",
    "loss_fn = nn.MarginRankingLoss(margin=1,size_average=False)\n",
    "\n",
    "# Sampler\n",
    "data = {\n",
    "    \"question\":train_question[:5000],\n",
    "    \"positive_path\": train_positive_path[:5000],\n",
    "    \"negative_path\": train_negative_path[:5000]\n",
    "}\n",
    "\n",
    "ss = SimplestSampler(data,parameter_dict['batch_size'])\n",
    "\n",
    "\n",
    "# Training\n",
    "training_loop(parameter_dict=parameter_dict, data_sampler=ss, ranker=model, \n",
    "              optimizer=optimizer, loss_func=loss_fn, test_data=test_data,device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['node', 'parsed_sparql', 'path', 'entity', 'constraints', 'updated_sparql', 'hop1', 'hop2', 'error_flag', 'rdf_constraint', 'hop1_sf', 'hop2_sf', 'path_sf', 'question_id', 'negative_paths_id', 'positive_path_id'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[0].keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
