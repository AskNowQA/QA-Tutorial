{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file outlines the method for creating an **embedding based** question answering system.\n",
    "Following are the major steps followed\n",
    "* Find the entity\n",
    "* Create a set of core chain candidates\n",
    "* Rank the core chain candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports (external library)\n",
    "import json\n",
    "import numpy as np\n",
    "import requests\n",
    "from pprint import pprint\n",
    "import create_data_node as cdn\n",
    "from utils import natural_language_utilities as nlutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entity linking\n",
    "\n",
    "We employ [EARL](http://sda.cs.uni-bonn.de/projects/earl/) which returns a set of candidate entities of which we use the top most one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://dbpedia.org/resource/India']\n"
     ]
    }
   ],
   "source": [
    "def get_entities(question,show_internals=False):\n",
    "    \"\"\"\n",
    "        uses EARL to find all the entites present in the question.\n",
    "        :param question: a natural language question.\n",
    "        :return: entities list.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "    }\n",
    "\n",
    "    data = '{\"nlquery\":\"%(p)s\"}'% {\"p\":question}\n",
    "    response = requests.post(' http://asknow02.sda.tech/earl/api/processQuery', headers=headers, data=data)\n",
    "    a = json.loads(response.content)\n",
    "    if show_internals:\n",
    "        pprint(a)\n",
    "    entity_list = []\n",
    "    for i in range(len(a['ertypes'])):\n",
    "        if a['ertypes'][i] == 'entity':\n",
    "            entity_list.append(a['rerankedlists'][str(i)][0][1]) # return the top most one. \n",
    "    return entity_list\n",
    "\n",
    "print(get_entities('Who is the president of India ?',False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sub Graph creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Cache not found. Creating a new one\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/gaurav/codes/QA-Tutorial/utils/dbpedia_interface.py\", line 137, in __init__\n",
      "    self.labels = pickle.load(open('resources/labels.pickle'))\n",
      "TypeError: a bytes-like object is required, not 'str'\n"
     ]
    }
   ],
   "source": [
    "# predicate blacklist to exclude some of the meta data information\n",
    "pb = open('resources/predicate.blacklist').readlines()\n",
    "pb[-1] = pb[-1] + '\\n'\n",
    "pb = [r[:-1] for r in pb]\n",
    "\n",
    "\n",
    "cd_node = cdn.CreateDataNode(_predicate_blacklist=pb, _relation_file={}, _qald=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hop1,hop2  = cd_node.create_subgraph.subgraph\\\n",
    "            (['http://dbpedia.org/resource/Michael_Crichton'],[],_use_blacklist=True,_qald=False)\n",
    "\n",
    "if False:\n",
    "    print('few examples of hop1 candidates')\n",
    "    pprint(hop1[:5])\n",
    "    print('few examples of hop2 candidates')\n",
    "    pprint(hop2[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(query):\n",
    "    query_json = {'question':query}\n",
    "    v = requests.get(\"http://localhost:3500/vec\", json=query_json)\n",
    "    v = np.asarray(v.json())\n",
    "    v = np.mean(v.astype(np.float), axis=0)\n",
    "    return v\n",
    "\n",
    "\n",
    "def assign_score(core_chain,question):\n",
    "    if len(core_chain) == 2:\n",
    "        # corechain looks like ['+', 'http:.../abc']\n",
    "        predicate = nlutils.get_label_via_parsing(core_chain[1],lower=True)\n",
    "    else:\n",
    "        # corechain looks like ['+' 'http:../abc', '-', 'http:../pqr']\n",
    "        predicate = [nlutils.get_label_via_parsing(core_chain[1],lower=True),\n",
    "                     nlutils.get_label_via_parsing(core_chain[3],lower=True)]\n",
    "        predicate = \" \".join(predicate)\n",
    "    \n",
    "    question_vector = get_vector(query=question)\n",
    "    predicate_vector = get_vector(query=predicate)\n",
    "    \n",
    "    if np.sum(question_vector) == 0.0 or np.sum(predicate_vector) == 0:\n",
    "        return np.float64(0.0)\n",
    "    else:\n",
    "        print('here')\n",
    "        return np.dot(predicate_vector, question_vector) / (np.linalg.norm(predicate_vector) *\n",
    "                                                            np.linalg.norm(question_vector))\n",
    "    \n",
    "    # predicate is a string seperated by space -- 'abc pqr'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading dataset. \n",
    "\n",
    "The dataset consists of 2000 questions without any rdf constraint or count or ask. It is just composed of single hop or two hop query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = json.load(open('resources/dataset_with_paths.json'))\n",
    "\n",
    "\n",
    "# query = {'question':'Who is the president of India ?'}\n",
    "# v = requests.get(\"http://localhost:3500/vec\", json=query)\n",
    "# v = np.asarray(v.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "The code snippets for evaluating sparql with respect to ground truth sparql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparql_answer(sparql,dbi=None):\n",
    "    '''\n",
    "        Executes the sparql on dbpedia and returns answer as a list.\n",
    "        :param sparql: SPARQL which will be executed.\n",
    "        :param dbi: The dbpedia interface object which can be used for accesing dbpedia.\n",
    "    '''\n",
    "    if not dbi:\n",
    "        dbi = dbp\n",
    "    test_answer = []\n",
    "    interface_test_answer = dbi.get_answer(sparql)\n",
    "    for key in interface_test_answer:\n",
    "        test_answer = test_answer + interface_test_answer[key]\n",
    "    return list(set(test_answer))\n",
    "\n",
    "\n",
    "def _evaluate_sparqls_(test_sparql, true_sparql, type, ground_type,dbp):\n",
    "    # @TODO: If the type of test and true are differnt code would return an error.\n",
    "    \"\"\"\n",
    "        Fmeasure for ask and count are 0/1.\n",
    "        Also assumes the variable to be always uri.\n",
    "        :param test_sparql: SPARQL generated by the pipeline\n",
    "        :param true_sparql: True SPARQL\n",
    "        :param type: COUNT/ASK/LIST\n",
    "        :return: f1,precision,recall\n",
    "    \"\"\"\n",
    "    test_answer = sparql_answer(test_sparql,dbp)\n",
    "    true_answer = sparql_answer(true_sparql,dbp)\n",
    "    total_retrived_resutls = len(test_answer)\n",
    "    total_relevant_resutls = len(true_answer)\n",
    "    common_results = total_retrived_resutls - len(list(set(test_answer ) -set(true_answer)))\n",
    "    if total_retrived_resutls == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = common_results *1.0 /total_retrived_resutls\n",
    "    if total_relevant_resutls == 0:\n",
    "        recall = 0\n",
    "    else:\n",
    "        recall = common_results *1.0 /total_relevant_resutls\n",
    "    if precision == 0 and recall == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = (2.0 * (precision * recall)) / (precision + recall)\n",
    "    return f1 ,precision ,recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.4739299003444137"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_chain = dataset[0]['hop1'][1]\n",
    "question = 'Who is the president of India ?'\n",
    "assign_score(core_chain,question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+', 'http://dbpedia.org/ontology/office']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "core_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
